{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Questions for Yevgen",
      "metadata": {
        "tags": [],
        "cell_id": "00000-177b254f-1b21-43fb-ba18-2b870ce5a206",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- [ ] Why do we divide the `KLP` with `neff`\n- [ ] You take the mean of `RL + beta*KLZ` in the full loss. Why? And doesn't that mean it should be changed in the `train.py`  \n- [ ] ask about the variance - why is it 2\n- [ ] why is the logvar constant - why is it -5\n",
      "metadata": {
        "tags": [],
        "cell_id": "00001-52e1ce91-20e5-4b1c-8d96-6822edd46fd2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Notes meeting 15/04\nLoss has three components\nLots of weights we have to scale down, make the weights as much normal as possible \n(second option: number of the full dataset)\n\nNeff, should around 2thousand and something\n\nAnother function for evaluate the logP - calculate the probability (not looking at the parameters?) using a modified loss (we don’t for back.. but see the correlation) (we get the diff back)….if we do basin we need to do it reassemble, cause we need to reduce the variance \n\n\n`mt_elbos` # mutant elbos\nIt return the expected values\n\n**Third picture** we evaluate the network and has to be implemented in the train.py\n\n**Fourth picture** is taken form the init function \n\n**Fifth picture**\n- (up left) sum of reconstruction loss\n- (up right) useful to check if code is correct \n- (correlation) took not all the epoch but some of them (huge calculations) in the end it converges and reach the result mentioned in the paper\n\nQuestions:\n- Last epoch might be have a lower number of samples (related in the picture with the error evaluation KLP)\n- variance: something in the article? or trick to make it work (init values away from 0)\n- C: is for compression (we have lot of information in W, and narrow it down)\n- make_variational_linear(): we want our own control of the weights, we register two parameters - this is our prior for the weights, we sample from the distribution before the propagation (we have to call it once….?)\n- why we sum only the last layer/dimension `.sum(-1)`, KLP is a vector ?????? Didn’t understand what he said tbh\n\nMore stuff to include later (a little bit…):\n- batch normalisation\n- narrow down with the C\n\nRegarding the presentation/exam:\n- Know sparsity??? Turn off some of the positions?\n- Bayesian, to regularise and make it more general ",
      "metadata": {
        "tags": [],
        "cell_id": "00002-d677da0b-59ea-4544-960e-29e500371fc5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-b09bf01b-4631-4889-936b-6f2a119015fc",
        "deepnote_cell_type": "code"
      },
      "source": "def loss(self, recon_x, x, mu, logvar):\n    # RL, Reconstruction loss\n    # RL = F.nll_loss(recon_x, x.argmax(dim=1), reduction='none).sum(-1)\n    RL = (-recon_x*x).sum(-2).sum(-1)\n    # RL2 = F.nll_loss(recon_x, x.argmax(dim=1), reduction='none').sum(-1)\n\n    # KL, Kullback-Leibler divergence loss (for `z`):\n    # KLZ = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(-1)\n    KLZ_q = Normal(mu, logvar.mul(1/2).exp())\n    KLZ_p = Normal(torch.zeros_like(mu), torch.ones_like(logvar))\n    KLZ   = kl_divergence(KLZ_q, KLZ_p).sum(-1)\n\n    # KLP, Kullback-Leibler divergence loss (for network parameters):\n    KLP = 0\n    for l in self.variational_layers:\n        weight_mu, weight_std = l.weight_mean, l.weight_logvar.mul(1/2).exp()\n        q_weight = Normal(weight_mu, weight_std)\n\n        if l.name == 'S':\n            p_weight = Normal(torch.zeros_like(weight_mu) - 9.305, torch.zeros_like(weight_std) + 4)\n        else:\n            p_weight = Normal(torch.zeros_like(weight_mu), torch.ones_like(weight_std))\n        \n        KLP += kl_divergence(q_weight, p_weight).sum()\n\n        if l.bias is None: continue\n\n        bias_mu, bias_std = l.bias_mean, l.bias_logvar.mul(1/2).exp()\n        q_bias = Normal(bias_mu, bias_std)\n        p_bias = Normal(torch.zeros_like(bias_mu), torch.ones_like(bias_std))\n        KLP += kl_divergence(q_bias, p_bias).sum()\n\n\n    # Variational parameters: lambda (lamb) and w_out_b\n    lamb_mu, lamb_std = self.lamb_mean, self.lamb_logvar.mul(1/2).exp()\n    q_lamb = Normal(lamb_mu, lamb_std)\n    p_lamb = Normal(torch.zeros_like(lamb_mu), torch.ones_like(lamd_std))\n    KLP   += kl_divergence(q_lamb, p_lamb).sum()\n\n    W_out_b_mu, W_out_b_std = self.W_out_b_mean, self.W_out_b_logvar.mul(1/2).exp()\n    q_W_out_b = Normal(W_out_b_mu, W_out_b_std)\n    p_W_out_b = Normal(torch.zeros_like(W_out_b_mu), torch.ones_like(W_out_b_std))\n    KLP      += kl_divergence(q_W_out_b, p_W_out_b).sum()\n\n    KLP /= self.neff\n    loss = (RL + self.beta * KLZ).mean() + KLP\n\n    KLP = torch.tensor([0], requires_grad=False) if KLP == 0 else KLP\n    return loss, RL.mean(), KLZ.mean(), KLP\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-7c10f15f-bbdf-4217-b8a0-f044b7795641",
        "deepnote_cell_type": "code"
      },
      "source": "def logp(self, batch, rand = False):\n    ''' Returns individual log likelihoods of the batch'''\n\n    mu, logvar = self.encode(batch)\n    z          = self.reparameterize(mu, logvar) if rand else mu\n\n    recon      = self.decode(z)\n    recon      = recon.view(-1, self.alphabet_len, self.seq_len)\n    recon      = recon.log_softmax(dim=1)\n\n    # logp     = F.nll_loss(recon, batch.argmax(dim=1), reduction='none').sum(-1)\n    logp       = (-recon*batch).sum(-2).sum(-1)\n    kl         = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(-1)\n    elbo       = logp + kl\n\n    return elbo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-a171f3cb-2741-49db-9e7a-0e65b485c698",
        "deepnote_cell_type": "code"
      },
      "source": "def get_cor_ensamble(batch, mutants_values, model, ensambles = 512, rand = True):\n    model.eval()\n\n    mt_elbos, wt_elbos = 0, 0\n\n    for i in range(ensambles):\n        if i and (i % 2 == 0):\n            print(f\"\\tReached {i}/rand={rand}\", \" \"*32, end=\"\\r\")\n\n        elbos     = model.logp(batch, rand=rand).detach().cpu()\n        wt_elbos += elbos[0]\n        mt_elbos += elbos[1:]\n\n    print()\n\n    diffs  = (mt_elbos / ensambles) - (wt_elbos / ensambles)\n    cor, _ = spearmanr(mutants_values, diffs)\n    \n    return cor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "----\n**Code sent on zoom chat**",
      "metadata": {
        "tags": [],
        "cell_id": "00011-6523ca1c-f1c2-4fab-9e4e-dcc3f670d169",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-0743b1b1-14d7-427a-a6c3-76faf67f884a",
        "deepnote_cell_type": "code"
      },
      "source": "# Loss() in vae.py → \n\nfrom torch.distributions import kl_divergence\nKLZ_q = Normal(mu, logvar.mul(1/2).exp())\nKLZ_p = Normal(torch.zeros_like(mu), torch.ones_like(logvar))\nKLZ   = kl_divergence(KLZ_q, KLZ_p).sum(-1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-4253327d-7847-4899-b7ae-a340df426533",
        "deepnote_cell_type": "code"
      },
      "source": "# Loss of recostration - distance network results and expected\n# we have categorical distribution (for every distribution??)\n# every column is one position\n\nfrom torch.nn import functional as F\nF.nll_loss(recon_x, x.argmax(dim=1), reduction='none').sum(-1)\n\n# we don’t do any reduction - otherwise mess up with the ….tensor?",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "**super messy notes**\n\nLoss() in vae.py → \n\n```\nfrom torch.distributions import kl_divergence\nKLZ_q = Normal(mu, logvar.mul(1/2).exp())\nKLZ_p = Normal(torch.zeros_like(mu), torch.ones_like(logvar))\nKLZ   = kl_divergence(KLZ_q, KLZ_p).sum(-1)\n```\n\n\nLoss of recustring - distance network results and expected\nwe have categorical distribution (for every distribution??)\nevery column is one position\n\n```\nfrom torch.nn import functional as F\nF.nll_loss(recon_x, x.argmax(dim=1), reduction='none').sum(-1)\n```\n\nwe don’t do any reduction - otherwise mess up with the ….tensor?\n\n...if we do in log...\n\n\n→ Sparsity? → group sparsity prior: L1 Penalty? → MATRIX!! Hidden unity fully connected\nTurn connection, does not affect… [somthing] multiply on top on hidden layer\n\n\nWE WANT THE FUNKY PART \n\n[..]\n\nlook at similarities, calculate similarities, reduce their weight on the train\n\ncreated a layer [look at code shared on slack]\nlambda has baias - this is not probabilistic \n\n[ picture2 ] in the constructor of VAE --- we have to do some tricks (for?)\n\n[picture3 it also includes the lambda] multiply matrices ….and?extract matrix from the layer, do manipulations, → it’s the calculation of the 3 matrices → the matrix that we learn (...is going to be sparse,... turn on the weight)\n\nPRINT OUT THE TENSOR → FIGURE OUT THE SHAPE OF STUFF (this step to learn and understand)\n\n[picture 4] forward inside the encoder method - it returns mean, …., latent space\n\nSTEFAN QUESTION: Right before the last layer (in the encoder)\n\n[Eugenio didn’t do the ensembling]\n\nSequenceRate\ncalculate huge matrix with similarities (hamming distance)\nlook at how similar is protein, a comparison between them\nwe can calculate this in batches (this will speed up the creation of the matrix)\n\n(new picture)we’ll get a list of weights ---> and retrain???? not sure about this..\nit fully deterministic\n[what we have to do then?]\nHINT: data loader, combined with a sampler (look at new picture)\n\n```\nfrom torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n.backward(<<< Parameter >>>) // used for scale\n```\n\n\n“Second, we encourage correlation between amino acid usage, by convolving the final layer with a width-1 convolution operation.” → this is the dictionary → the DEMATRIX?\n\n\nSCALE UP THE NETWORK (for sparsity) with more hidden layer\n\nlinear layer is a fully connected layer ;)\n",
      "metadata": {
        "tags": [],
        "cell_id": "00014-e51f9fa8-8d06-4029-bab8-4612e8095cc5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=35838f82-2ce6-4453-9bd2-2d87a43af151' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "50171d3c-85e5-40f1-91b6-15cd87369a3c",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}